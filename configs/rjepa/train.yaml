# -------------------------------------------------------------------------------
# R-JEPA Training Configuration
# -------------------------------------------------------------------------------

# Model architecture (ORIGINAL SIZE - testing if OOM still occurs)
model:
  dim: 4096  # INPUT dimension (raw latent size from Qwen3-8B layer -2)
  encoder_embed_dim: 2048  # ENCODER dimension (ORIGINAL)
  depth_encoder: 6  # Encoder depth (ORIGINAL)
  depth_predictor: 4  # Predictor depth (ORIGINAL)
  num_heads: 8  # Number of attention heads (ORIGINAL)
  predictor_dim: 1024  # Predictor internal dimension (ORIGINAL)
  mlp_ratio: 4.0  # MLP expansion ratio (ORIGINAL)
  dropout: 0.0  # Dropout (0.0 for deterministic training)
  max_steps: 512  # Maximum sequence length
  domain_embed_dim: 64  # Domain embedding dimension (ORIGINAL)
  num_domains: 50  # Number of domain classes
  ema_momentum: 0.996  # Initial EMA momentum (will be annealed)

  # Loss configuration
  loss:
    loss_type: "l1"  # L1 or L2
    var_reg_weight: 0.01  # Variance regularization weight
    var_reg_target: 1.0  # Target variance
    contrastive_weight: 0.1  # Contrastive loss weight (ACTIVE by default)
    contrastive_temperature: 0.07
    use_hard_negatives: true  # Use hard negatives from incorrect CoTs

# Masking strategy
masker:
  type: "contiguous"  # "random", "contiguous", "hierarchical"
  min_mask_ratio: 0.3
  max_mask_ratio: 0.7
  num_blocks: 1

# Data paths
data:
  train_latents_dir: "data/latents/qwen3-8b/academic_shards"
  val_latents_dir: "data/latents/qwen3-8b/academic_shards"

# Training configuration
training:
  batch_size: 1  # Batch size (CRITICAL: Reduced to 1 to avoid OOM with shard caching!)
  accumulation_steps: 8  # Gradient accumulation (effective batch_size = 1*8 = 8)
  max_epochs: 100  # Total epochs
  lr: 3.0e-4  # Learning rate
  weight_decay: 0.05  # Weight decay
  warmup_epochs: 10  # Warmup epochs
  grad_clip: 1.0  # Gradient clipping
  amp_enabled: true  # Automatic Mixed Precision
  ema_momentum_start: 0.996  # Initial EMA momentum
  ema_momentum_end: 0.9999  # Final EMA momentum (annealed)
  num_workers: 0  # WINDOWS LIMITATION: num_workers>0 causes GIL crashes with CUDA
                  # Must use 0 on Windows. On Linux, can use 4+ workers safely.
  log_interval: 10  # Log every N steps
  val_interval: 1  # Validate every N epochs

# W&B logging (optional)
wandb:
  enabled: false  # Set to true to enable W&B
  project: "rjepa-training"
  run_name: "rjepa-qwen3-8b-base"

# Device
device: "cuda"  # "cuda" or "cpu"
